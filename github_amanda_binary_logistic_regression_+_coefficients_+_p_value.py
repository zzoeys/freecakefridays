# -*- coding: utf-8 -*-
"""github - Amanda - binary logistic regression + coefficients + p-value.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dFjrdCzxr3Yz2ST6Ozn5ffQ_Qro6Ig_c
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt

table = pd.read_csv("data/polling_site_data/clean_precincts_with_polling_site.csv")
table.head()

table = pd.get_dummies(table,drop_first=True)
table.sample(5)

"""Firstly, in order to do binary logistic regression on the dataset, the dataset will need to be trained[(Li, 2017)](https://https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8#:~:text=Logistic%20Regression%20is%20a%20Machine,%2C%20failure%2C%20etc.)because the dataset contains a large number of samples. """

X_train, X_test, y_train, y_test = train_test_split(table.drop('Polling Sites', axis=1), table['Polling Sites'])

"""In this section below, """

LogReg = LogisticRegression()
LogReg.fit(X_train,y_train)

table = pd.read_csv("data/polling_site_data/clean_precincts_with_polling_site.csv")
table.head()

"""Creating the logit(odd) curve. Using Black population density as an example."""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,12]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

"""Building regression model and getting coefficients."""

import statsmodels.api as sm
table.head()

model = sm.GLM.from_formula("table.iloc[:,12] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

"""Now we find the p-value using the z-value

"""

import scipy.stats 
scipy.stats.norm.sf(abs(-3.664))

"""Now we are going to find the coefficients for all of the racial groups.

1. Hispanic
"""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,10]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,10] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(-0.119))

"""2. White"""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,11]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,11] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(4.132))

"""3. Black"""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,12]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,12] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(-3.664))

"""4. Asian"""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,13]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,13] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(-1.370))

"""5. Mixed"""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,14]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,14] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(-0.257))

"""6. Others """

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,15]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,15] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(-0.480))