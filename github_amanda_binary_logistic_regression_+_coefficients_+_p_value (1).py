# -*- coding: utf-8 -*-
"""github - Amanda - binary logistic regression + coefficients + p-value.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dFjrdCzxr3Yz2ST6Ozn5ffQ_Qro6Ig_c
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt

table = pd.read_csv("data/polling_site_data/clean_precincts_with_polling_site.csv")
table.head()

table = pd.get_dummies(table,drop_first=True)
table.sample(5)

"""Firstly, in order to do binary logistic regression on the dataset, the dataset will need to be trained[(Li, 2017)](https://https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8#:~:text=Logistic%20Regression%20is%20a%20Machine,%2C%20failure%2C%20etc.)because the dataset contains a large number of samples. """

X_train, X_test, y_train, y_test = train_test_split(table.drop('Polling Sites', axis=1), table['Polling Sites'])

"""In this section below, the trained dataset can then be used to produce logistic regression using the sklearn logistic regression package [(Galarnyk, 2017)](https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a). """

LogReg = LogisticRegression()
LogReg.fit(X_train,y_train)

table = pd.read_csv("data/polling_site_data/clean_precincts_with_polling_site.csv")
table.head()

"""Now, after setting up the dataset for logistic regression, we will create a log(odd) plot. This step converts the previous binary logistic regression we produced in python (although not visualised) that could have been shown in Sigmoid curve if visualised, into log(odd) plot which is displayed similar to a linear model. Once the we have the log(odd) plot, we can obtain the coefficients and p-value for regression analysis. The mathematical concept behind this step is explained in "Methodology" and "Data Analysis" section."""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,12]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

"""Now we will use the statsmodel.api library to obtain the coefficients for the log(odd) plots"""

import statsmodels.api as sm
table.head()

model = sm.GLM.from_formula("table.iloc[:,12] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

"""Now we find the p-value using the z-score using the scipy library

"""

import scipy.stats 
scipy.stats.norm.sf(abs(-3.664))

"""Now, the step for producing the log(odd) plot, finding the coefficients and finding the p-value using the z-score is repeated for each racial group as demonstrated below. There is no need to repeat the step of training the dataset since it has already been done for the entire dataset.

1. Hispanic
"""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,10]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,10] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(-0.119))

"""2. White"""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,11]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,11] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(4.132))

"""3. Black"""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,12]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,12] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(-3.664))

"""4. Asian"""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,13]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,13] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(-1.370))

"""5. Mixed"""

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,14]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,14] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(-0.257))

"""6. Others """

def model (x):
  return 1/(1+np.exp(-x))
x1 = np.linspace(0,1,num=400)
y1 = model(x1)

 
x = table.iloc[:,15]
y = table.iloc[:,16]
print(x)
model = LogisticRegression(solver='liblinear', random_state=0)
plt.scatter(x,y)
plt.scatter(x1,y1)
plt.show()

import statsmodels.api as sm

model = sm.GLM.from_formula("table.iloc[:,15] ~ table.iloc[:,16]", family = sm.families.Binomial(), data=table)
result = model.fit()
result.summary()

import scipy.stats 
scipy.stats.norm.sf(abs(-0.480))